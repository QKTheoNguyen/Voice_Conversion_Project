{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move files to speakers directory, save them as wav files and resample them to 24000 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import silero_vad\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"C:/Users/quang/Desktop/Dataset/Voice_Conversion/Data_common_voice/data\")\n",
    "data_clips_dir = Path(\"C:/Users/quang/Desktop/Dataset/Voice_Conversion/Data_common_voice/clips\")\n",
    "speaker_path = Path(\"C:/Users/quang/Desktop/Dataset/Voice_Conversion/Data_common_voice/speakers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(waveform, model):\n",
    "\n",
    "    speech_timestamps = silero_vad.get_speech_timestamps(\n",
    "                        waveform,\n",
    "                        model,\n",
    "                        return_seconds = False,\n",
    "                        sampling_rate = 32000\n",
    "                        )\n",
    "    \n",
    "    start_sample = int(speech_timestamps[0][\"start\"])\n",
    "    end_sample = int(speech_timestamps[-1][\"end\"]) + 6000\n",
    "\n",
    "    if end_sample >= waveform.shape[1]:\n",
    "        end_sample = waveform.shape[1]\n",
    "    if start_sample >= waveform.shape[1]:\n",
    "        start_sample = 0\n",
    "\n",
    "    # waveform = waveform[:, start_sample:end_sample]\n",
    "\n",
    "    try:\n",
    "        waveform = waveform[:, start_sample:end_sample]\n",
    "    except:\n",
    "        waveform = waveform\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Currently silero VAD models support 8000 and 16000 (or multiply of 16000) sample rates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39msample_rate, new_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24000\u001b[39m)(waveform)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# cut data\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43mclean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# save audio file in wav format\u001b[39;00m\n\u001b[0;32m     24\u001b[0m torchaudio\u001b[38;5;241m.\u001b[39msave(file_target, waveform, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24000\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mclean_data\u001b[1;34m(waveform, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_data\u001b[39m(waveform, model):\n\u001b[1;32m----> 3\u001b[0m     speech_timestamps \u001b[38;5;241m=\u001b[39m \u001b[43msilero_vad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_speech_timestamps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_seconds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24000\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     start_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(speech_timestamps[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     10\u001b[0m     end_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(speech_timestamps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m6000\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\quang\\anaconda3\\lib\\site-packages\\silero_vad\\utils_vad.py:274\u001b[0m, in \u001b[0;36mget_speech_timestamps\u001b[1;34m(audio, model, threshold, sampling_rate, min_speech_duration_ms, max_speech_duration_s, min_silence_duration_ms, speech_pad_ms, return_seconds, visualize_probs, progress_tracking_callback, neg_threshold, window_size_samples)\u001b[0m\n\u001b[0;32m    271\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_rate \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m8000\u001b[39m, \u001b[38;5;241m16000\u001b[39m]:\n\u001b[1;32m--> 274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently silero VAD models support 8000 and 16000 (or multiply of 16000) sample rates\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    276\u001b[0m window_size_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sampling_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    278\u001b[0m model\u001b[38;5;241m.\u001b[39mreset_states()\n",
      "\u001b[1;31mValueError\u001b[0m: Currently silero VAD models support 8000 and 16000 (or multiply of 16000) sample rates"
     ]
    }
   ],
   "source": [
    "speaker_csv = pd.read_csv(speaker_path)\n",
    "model = silero_vad.load_silero_vad()\n",
    "\n",
    "for index, row in speaker_csv.iterrows():\n",
    "    files = row[\"files\"].replace('[', '').replace(']','').replace(\"'\",\"\").replace(\" \",\"\").split(\",\")\n",
    "    count = row[\"count\"]\n",
    "    speaker_count = index + 1\n",
    "    speaker_string = \"{:03d}\".format(speaker_count)\n",
    "    if not os.path.exists(os.path.join(data_dir, speaker_string)):\n",
    "        os.makedirs(os.path.join(data_dir, speaker_string))\n",
    "    \n",
    "    for file in files:\n",
    "        file_source = os.path.join(data_clips_dir, file)\n",
    "        file_target = os.path.join(data_dir, speaker_string, file).replace(\".mp3\", \".wav\")\n",
    "\n",
    "        # open mp3 audio_file\n",
    "        # waveform, sample_rate = librosa.load(file_source, sr=None)\n",
    "        waveform, sample_rate = torchaudio.load(file_source)\n",
    "        # cut data\n",
    "        waveform = clean_data(waveform, model)\n",
    "        # resample audio to 24000 Hz\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=24000)(waveform)\n",
    "        # save audio file in wav format\n",
    "        torchaudio.save(file_target, waveform, sample_rate=24000, format=\"wav\")\n",
    "\n",
    "    if count <= 10: break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
